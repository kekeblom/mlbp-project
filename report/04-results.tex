I tried diferent network depths from 1 to 4 layers deep along with different layer widths. I also tried a number of different activation functions for all of the layers namely tanh, sigmoid and rectified linear units.

In the initial experiments the models tended to plateau around a final test accuracy of about 62\%. I tried to adjust all of the parameters including learning rate but the models would get stuck in that plateau. This got me thinking the model migth be suffering from overfitting and therefore might benefit from some regularization scheme. I added the weighted l2 norm of the weights as a regularization term to the cost function. This substantially improved the final test set accuracy. After trying out different weights I settled for a weight of 0.5.

The network that turned out to perform the best was a two layer network
